. Make photon skims
  misc/photonskim.py

. Compute the luminosity (uses source simpletree)
  misc/lumilist.py <samples> [--mask <lumimask>] --save --save-plain

. Update the number of events for the data samples (technically not necessary)
  python datasets.py recalculate <samples> --save

. Compute the data pileup distribution and construct the PU weights
  pileupCalc.py -i data/lumis_plain.txt --inputLumiJSON /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions16/13TeV/PileUp/pileup_latest.txt --calcMode true --minBiasXsec 71300 --maxPileupBin 80 dataPileup.root
  misc/puweight.py dataPileup.root 2016_25ns_SpringMC_PUScenarioV1_PoissonOOTPU $PWD/data/pileup.root
  !!! puweight.py is currently hacked to cap the reweight factor at 4

. Run the trigger efficiency measurement and update the L1 fit parameters in selectors.py
  condor-run $PWD/trigger/skim.py -j "electron sel-X" "muon smu-X"
  condor-run $PWD/trigger/skim.py -e photon -j sel-X jht-X
  trigger/eff.py sel ptzoom hlt
  trigger/eff.py jht pt l1

. Run the photon ID and lepton veto efficiency measurements. Propagate the SF and uncertainty to selectors.py
  condor-run $PWD/veto_eff/skim.py dimu -j smu-16X dy-50 tt ww wz zz
  veto_eff/skim.py monoph znng
  veto_eff/compute.py

. At this point, all the weights for MC samples have been updated and you can reskim all the MC samples (but don't yet)

. Run the e->photon fake rate study. Copy the output of compute to data/efake_data_pt.root
  condor-run $PWD/tp/skim.py -j sel-X smu-X dy-50 gg-80 tt wlnu ww wz zz
  tp/efake_fit.py mc highpt
  tp/efake_fit.py data highpt
  condor-run $PWD/tp/efake_fit.py -e "(data|mc) highpt 1000 (ee|eg) pt_100_6500" -j $(seq 1 200)
  tp/efake_compute.py (data|mc) highpt # make sure toyUncert is set to True

. Run the purity and hfake transfer factor measurements. Update data/hadronTFactor.root

. Reskim efake and hfake samples with updated fake rates and transfer factors.

. Run method 1 and 2 of the gamma + jets background estimation.

. At this point we need to run the skim for data once - halo estimate fits on candidate events.

. Run the halo phi fit and update ssw2.py (haloNorms) - TODO change this to transfer factor in selectors.py
  halo/phidistributions.py for plots
  halo/phifit.py for actual fits

. Run the sph skim again

. Run method 3 of gamma + jets background estimation.
  gjets/smearfit.py

. Run gj-* skim again, after updating gjsmear parameters in selectors.py

. Run all other MC skims (needed if PU reweighting changed)

. Make plots and data card inputs
  main/plot.py monoph
  main/plot.py -p met -o monoph_met.root --allsignal

. Make data cards
  main/datacard.py {model} monoph_met.root
  for model in ; do python datacard.py $model monoph_met.root; done

. Run combine
  scratch/runCombine.sh {point}
  for f in /scratch5/ballen/hist/monophoton/datacards/*; do echo $f >> cards.txt; done
  ~/bin/condor-run scratch/runCombine.sh -a cards.txt

. Fix scaled samples
  scratch/fixLimit.py

. Plot limits
  scratch/plotlimit.py
